# TechGPT 2.0: Technology-Oriented Generative Pretrained Transformer 2.0
Demo: [TechGPT-neukg](http://techgpt.neukg.com) <br>
HuggingFaceğŸ¤—: [neukg/TechGPT-7B](https://huggingface.co/neukg)

<div align="center">

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/neukg/TechGPT/blob/main/LICENSE)
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/neukg)
</div>

## å¼•è¨€
éšç€å¤§æ¨¡å‹æ—¶ä»£çš„åˆ°æ¥ï¼Œå¤§æ¨¡å‹ä¸çŸ¥è¯†å›¾è°±èåˆçš„å·¥ä½œæ—¥ç›Šæˆä¸ºå½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚ä¸ºäº†å¯¹è¿™é¡¹å·¥ä½œæä¾›ç ”ç©¶çš„åŸºç¡€ï¼Œä¸œåŒ—å¤§å­¦çŸ¥è¯†å›¾è°±ç ”ç©¶ç»„æ­¤å‰å‘å¸ƒäº†[TechGPT-1.0](https://github.com/neukg/TechGPT)å¤§æ¨¡å‹ã€‚ç»è¿‡äº†å‡ ä¸ªæœˆçš„å·¥ä½œï¼Œä¸œåŒ—å¤§å­¦çŸ¥è¯†å›¾è°±ç ”ç©¶ç»„å‘å¸ƒ**TechGPT-2.0**å¤§æ¨¡å‹ï¼Œå…±åŒ…å«ä¸¤ä¸ª7Bæ¨¡å‹ï¼Œå¹¶åŒæ—¶æ‰©å……äº†å¤šé¡¹èƒ½åŠ›ã€‚
## å†…å®¹å¯¼å¼•
| ç« èŠ‚                   | æè¿°                          |
|----------------------|-----------------------------|
| [ğŸ’ğŸ»â€â™‚ï¸æ¨¡å‹ç®€ä»‹](#æ¨¡å‹ç®€ä»‹) | ç®€è¦ä»‹ç»æœ¬é¡¹ç›® TechGPT 2.0 æ¨¡å‹çš„æŠ€æœ¯ç‰¹ç‚¹ |
| [ğŸ“æ¨¡å‹äº®ç‚¹](#æ¨¡å‹äº®ç‚¹)      | ä»‹ç»äº† TechGPT 2.0 å¤§æ¨¡å‹çš„ç‹¬ç‰¹ä¹‹å¤„    |
| [â¬æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)       | TechGPT 2.0 å¤§æ¨¡å‹ä¸‹è½½åœ°å€         |
| [ğŸ’»ç¯å¢ƒéƒ¨ç½²](#æ¨ç†ä¸éƒ¨ç½²)     | ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ä¸ªäººç¯å¢ƒéƒ¨ç½²å¹¶ä½“éªŒå¤§æ¨¡å‹         |
| [ğŸ’¯ç³»ç»Ÿæ•ˆæœ](#ç³»ç»Ÿæ•ˆæœ)      | ä»‹ç»äº†æ¨¡å‹åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šçš„æ•ˆæœ              |

## æ¨¡å‹ç®€ä»‹
TechGPT-2.0 ä¸º TechGPT-1.0 åŸºç¡€ä¸Šçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ­¤æ¬¡å…±å‘å¸ƒä¸¤ä¸ª7Bç‰ˆæœ¬çš„æ¨¡å‹åˆ†åˆ«ä¸º**TechGPT2-Alpaca**ã€**TechGPT2-Atom**ã€‚

TechGPT-2.0 è¾ƒ TechGPT-1.0 æ–°åŠ äº†è®¸å¤šé¢†åŸŸçŸ¥è¯†ã€‚é™¤äº† TechGPT-1.0 æ‰€å…·å¤‡çš„è®¡ç®—æœºç§‘å­¦ã€ææ–™ã€æœºæ¢°ã€å†¶é‡‘ã€é‡‘èå’Œèˆªç©ºèˆªå¤©ç­‰åä½™ç§å‚ç›´ä¸“ä¸šé¢†åŸŸèƒ½åŠ›ï¼ŒTechGPT-2.0 è¿˜åœ¨**åŒ»å­¦ã€æ³•å¾‹é¢†åŸŸ**å±•ç°å‡ºä¼˜ç§€çš„èƒ½åŠ›ï¼Œå¹¶æ‰©å……äº†**åœ°ç†åœ°åŒºã€è¿è¾“ã€ç»„ç»‡ã€ä½œå“ã€ç”Ÿç‰©ã€è‡ªç„¶ç§‘å­¦ã€å¤©æ–‡å¯¹è±¡ã€å»ºç­‘**ç­‰é¢†åŸŸèƒ½åŠ›ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¿˜å¯¹**å¹»è§‰ã€ä¸å¯å›ç­”é—®é¢˜ã€é•¿æ–‡æœ¬**ç­‰ä»»åŠ¡è¿›è¡Œäº†ç ”ç©¶ã€‚

**ä¸œåŒ—å¤§å­¦çŸ¥è¯†å›¾è°±ç ”ç©¶ç»„ä¸åä¸ºæ²ˆé˜³äººå·¥æ™ºèƒ½è®¡ç®—ä¸­å¿ƒ**åˆä½œï¼Œä½¿ç”¨**åä¸ºæ˜‡è…¾æœåŠ¡å™¨**ï¼ˆå…·ä½“ä¸º4æœº*8å¡-32G 910Aï¼‰è¿›è¡Œ**å…¨é‡å¾®è°ƒ**ã€‚

## æ¨¡å‹äº®ç‚¹
TechGPT-2.0 åœ¨ TechGPT-1.0 çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡è¦çš„æ”¹è¿›ï¼Œå…¶ä¸­æœ€æ˜¾è‘—çš„ä¼˜åŒ–æ˜¯å¼•å…¥äº†å‘½åå®ä½“è¯†åˆ«çš„é¢†åŸŸæ•°æ®ï¼Œå¹¶ä¸”ä½¿æ¨¡å‹å…·å¤‡äº†**å¯¹åµŒå¥—å®ä½“çš„æŠ½å–èƒ½åŠ›**ã€‚
- é¦–å…ˆï¼ŒTechGPT-2.0 åœ¨åŒ»å­¦é¢†åŸŸçš„è¡¨è¾¾å’Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ¨¡å‹èƒ½å¤Ÿå¯¹ç–¾ç—…ã€è¯ç‰©ã€ä¸“ä¸šæœ¯è¯­ç­‰å®ä½“çš„è¿›è¡Œæ›´å‡†ç¡®è¯†åˆ«ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨å¤„ç†åŒ»å­¦æ–‡æœ¬æ—¶èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒTechGPT-2.0 è¿˜èƒ½å¤Ÿç†è§£åŒ»å­¦æ–‡æœ¬ä¸­çš„å¤æ‚å…³ç³»ã€ç–¾ç—…è¯Šæ–­ã€æ²»ç–—æ–¹æ¡ˆç­‰å†…å®¹ã€‚è¿™ç§å…¨é¢çš„åŒ»å­¦åˆ†æèƒ½åŠ›ä½¿å¾—æ¨¡å‹å¯ä»¥ç”¨äºååŠ©åŒ»ç”Ÿé˜…è¯»åŒ»å­¦æ–‡çŒ®ã€æä¾›æ‚£è€…è¯Šæ–­å»ºè®®ç­‰åº”ç”¨åœºæ™¯ï¼Œä»è€Œæé«˜åŒ»å­¦é¢†åŸŸçš„ä¿¡æ¯å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
- å…¶æ¬¡ï¼ŒTechGPT-2.0 èƒ½å¤Ÿç†è§£å’Œè§£é‡Šæ³•å¾‹æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ³•è§„ã€åˆåŒå’Œæ¡ˆä¾‹æ³•ç­‰ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨æ›´ä¸ºå¹¿æ³›ï¼Œå¹¶å¯ä»¥ç”¨äºè§£å†³è‡ªåŠ¨åŒ–åˆåŒå®¡æŸ¥ã€æ³•è§„éµå¾ªæ£€æŸ¥ç­‰ä»»åŠ¡ã€‚æ¨¡å‹é€šè¿‡å­¦ä¹ æ³•å¾‹ç”¨è¯­å’Œç»“æ„ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰æ–‡æœ¬ä¸­çš„æ³•å¾‹å…³ç³»å’Œæ¡æ¬¾ï¼Œä¸ºç”¨æˆ·æä¾›æ›´æœ‰æ·±åº¦å’Œä¸“ä¸šæ€§çš„æ³•å¾‹åˆ†æã€‚
- å†æ¬¡ï¼ŒTechGPT 2.0 çš„å¦ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯èƒ½å¤ŸæŠ½å–åµŒå¥—å®ä½“ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥æ›´çµæ´»åœ°å¤„ç†å®ä½“ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œæ·±å…¥æŒ–æ˜æ–‡æœ¬ä¸­çš„å±‚æ¬¡ç»“æ„ï¼Œæé«˜äº†å¯¹å¤æ‚æ–‡æœ¬çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»å­¦æ–‡çŒ®ä¸­ï¼Œå¯èƒ½å­˜åœ¨åµŒå¥—çš„å®ä½“å…³ç³»ï¼Œå¦‚ç–¾ç—…çš„äºšå‹ã€è¯ç‰©çš„å‰‚é‡ä¿¡æ¯ç­‰ï¼ŒTechGPT-2.0 èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿™äº›ä¿¡æ¯ï¼Œå¹¶åœ¨ç”Ÿæˆå›åº”æ—¶æ›´å‡†ç¡®åœ°åæ˜ ä¸Šä¸‹æ–‡çš„è¯­å¢ƒã€‚
- æœ€åï¼ŒTechGPT-2.0 è¿˜å…·å¤‡è§£å†³å¹»è§‰å’Œäººç±»ä»·å€¼è§‚å¯¹é½çš„èƒ½åŠ›ã€‚æ¨¡å‹é€šè¿‡å¯¹è¯å’Œç†è§£ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»çš„æ„Ÿå—å’Œä»·å€¼è§‚ï¼Œå¹¶åœ¨å›åº”ä¸­è€ƒè™‘è¿™äº›å› ç´ ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»ç”¨æˆ·è¿›è¡Œäº¤äº’ï¼Œæ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚å’ŒæœŸæœ›ï¼Œè¿›ä¸€æ­¥æå‡äº†äººæœºäº¤äº’çš„è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚
æ€»ä½“è€Œè¨€ï¼ŒTechGPT 2.0 åœ¨ç»§æ‰¿äº† TechGPT 1.0 çš„å¼ºå¤§è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œé€šè¿‡å¢åŠ å¤šé¢†åŸŸã€å¤šä»»åŠ¡çš„æ•°æ®ï¼Œå±•ç°å‡ºäº†åµŒå¥—å®ä½“çš„æŠ½å–ã€å¹»è§‰å›ç­”ã€å›ç­”ä¸å¯å›ç­”é—®é¢˜å’Œå›ç­”é•¿æ–‡æœ¬é—®é¢˜çš„èƒ½åŠ›ã€‚è¿™äº›æ”¹è¿›ä½¿å¾—æ¨¡å‹æ›´é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ›´å‡†ç¡®ã€æ›´æ·±å…¥çš„ä¿¡æ¯å¤„ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚

## æ¨¡å‹ä¸‹è½½
| æ¨¡å‹åç§°                             |  ç±»å‹  | è®­ç»ƒæ–¹å¼ |  å¤§å°   |                              LoRAä¸‹è½½åœ°å€                              |
|:---------------------------------|:----:|:----:|:-----:|:------------------------------------------------------------------:|
| TechGPT-1.0                      | æŒ‡ä»¤æ¨¡å‹ | å…¨é‡å¾®è°ƒ | 13 GB | [[ğŸ¤—HF]](https://huggingface.co/neukg/TechGPT-7B)  |
| TechGPT-2.0-Alpaca ğŸ†•            | æŒ‡ä»¤æ¨¡å‹ | å…¨é‡å¾®è°ƒ | 13 GB |  [[ğŸ¤—HF]](https://huggingface.co/neukg/TechGPT-2.0-alpaca-hf)  |
| TechGPT-2.0-Atom ğŸ†•              | æŒ‡ä»¤æ¨¡å‹ | å…¨é‡å¾®è°ƒ | 13 GB | [[ğŸ¤—HF]](https://huggingface.co/neukg/TechGPT-2.0-atom-hf) |


## ç¯å¢ƒéƒ¨ç½²
### åœ¨åä¸ºæ˜‡è…¾ 910 NPU æœåŠ¡å™¨ä¸Šçš„ç¯å¢ƒè¦æ±‚
- ç¡¬ä»¶ï¼šAscend 910A/910B
- Pythonï¼š3.9
- MindSporeï¼š2.1.1
- CANN: 6.3.0 RC2
- MindFormersç‰ˆæœ¬ï¼šdev
- 7b æ¨ç†å¯åœ¨å•æœºå•å¡ä¸Šå®Œæˆéƒ¨ç½² 

1. åœ¨mindformersç¯å¢ƒä¸‹æ‰§è¡Œæ¨ç†éƒ¨ç½²æ—¶ï¼Œéœ€è¦ä½¿ç”¨ckptæƒé‡ï¼›å¦‚æœæ²¡æœ‰ckptæƒé‡ï¼Œåˆ™åœ¨mindformersç›®å½•ä¸‹éœ€è¦è¿è¡Œå¦‚ä¸‹è½¬æ¢è„šæœ¬ï¼Œå°†huggingfaceæƒé‡è½¬ä¸ºckptæƒé‡ï¼Œæ‰èƒ½ä½¿ç”¨NPUè¿›è¡Œæ¨ç†ï¼š
``` shell
python mindformers/models/llama/convert_weight.py \
--torch_ckpt_dir TORCH_CKPT_DIR \
--mindspore_ckpt_path {path}/MS_CKPT_NAME
```
2. åˆæ¬¡åœ¨mindformersç¯å¢ƒä¸‹æ‰§è¡Œæ¨ç†æ—¶ï¼Œä¼šåœ¨```mindspore_inference.py```çš„åŒçº§ç›®å½•ä¸‹ç”Ÿæˆ```checkpoint_download```æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«äº†æ¨ç†æ‰€éœ€çš„```yaml```é…ç½®æ–‡ä»¶å’Œ```tokenizer.model```è¯è¡¨ç­‰ï¼Œéœ€è¦å°†è¯è¡¨æ¢æˆè¯¥é¡¹ç›®huggingfaceä¸Šçš„å¯¹åº”è¯è¡¨ï¼Œå¹¶å°†é…ç½®æ–‡ä»¶æ›¿æ¢ä¸º```mindspore_inference```ç›®å½•ä¸‹çš„```yaml```æ–‡ä»¶ã€‚

### åœ¨ GPU æœåŠ¡å™¨ä¸Šçš„ç¯å¢ƒè¦æ±‚
è¯·æ³¨æ„TechGPT2-Alpacaå’ŒTechGPT2-Atomæ¨¡å‹åœ¨**è®­ç»ƒ**å’Œ**æ¨ç†**é˜¶æ®µæ‰€ä½¿ç”¨çš„promptæ ¼å¼æ˜¯ä¸åŒã€‚
**TechGPT2-Alpaca ä½¿ç”¨çš„promptæ ¼å¼ä¸ºï¼š**
``` python
<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [INST] <<SYS>>\nYou are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ä½ æä¾›ä¸“ä¸šã€æœ‰é€»è¾‘ã€å†…å®¹çœŸå®ã€æœ‰ä»·å€¼çš„è¯¦ç»†å›å¤ã€‚\n<</SYS>>\n\n{instruction} [/INST] ASSISTANT: 
```
**TechGPT2-Atom ä½¿ç”¨çš„promptæ ¼å¼ä¸ºï¼š**
``` python
<s>Human: {instruction} \n</s><s>Assistant: 
```

è¯·åœ¨ä½¿ç”¨TechGPTä¹‹å‰ä¿è¯ä½ å·²ç»å®‰è£…å¥½`transfomrers`å’Œ`torch`ï¼š

```shell
pip install transformers
pip install torch
```

- æ³¨æ„ï¼Œå¿…é¡»ä¿è¯å®‰è£…çš„ `transformers` çš„ç‰ˆæœ¬ä¸­å·²ç»æœ‰ `LlamaForCausalLM` ã€‚<br>
- Note that you must ensure that the installed version of `transformers` already has `LlamaForCausalLM`.

[TechGPT2-Alpca Example:](https://github.com/neukg/TechGPT/blob/main/inference.py)

``` python
from transformers import LlamaTokenizer, AutoModelForCausalLM, AutoConfig, GenerationConfig
import torch


DEFAULT_SYSTEM_PROMPT = """You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ä½ æä¾›ä¸“ä¸šã€æœ‰é€»è¾‘ã€å†…å®¹çœŸå®ã€æœ‰ä»·å€¼çš„è¯¦ç»†å›å¤ã€‚"""
TEMPLATE = (
    "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [INST] <<SYS>>\n"
    "{system_prompt}\n"
    "<</SYS>>\n\n"
    "{instruction} [/INST] ASSISTANT: "
)

def generate_prompt(instruction):
    return TEMPLATE.format_map({'instruction': instruction, 'system_prompt': DEFAULT_SYSTEM_PROMPT})

ckpt_path = './LLM/TechGPT2-Alpaca-hf/'
load_type = torch.float16
device = torch.device(1)
tokenizer = LlamaTokenizer.from_pretrained(ckpt_path)
tokenizer.pad_token_id = 0
tokenizer.bos_token_id = 1
tokenizer.eos_token_id = 2
tokenizer.padding_side = "left"
model_config = AutoConfig.from_pretrained(ckpt_path)
model = AutoModelForCausalLM.from_pretrained(ckpt_path, torch_dtype=load_type, config=model_config)
model.to(device)
model.eval()

generation_config = GenerationConfig(
    temperature=0.1,
    top_p=0.75,
    top_k=40,
    num_beams=1,
    bos_token_id=1,
    eos_token_id=2,
    pad_token_id=0,
    max_new_tokens=128,
    min_new_tokens=10,
    do_sample=True,
)

example = 'è¯·æŠŠä¸‹åˆ—æ ‡é¢˜æ‰©å†™æˆæ‘˜è¦, ä¸å°‘äº100å­—: åŸºäºè§†è§‰è¯­è¨€å¤šæ¨¡æ€çš„å®ä½“å…³ç³»è”åˆæŠ½å–çš„ç ”ç©¶ã€‚'

instruction = generate_prompt(instruction=example)

instruction = tokenizer(instruction, return_tensors="pt")
input_ids = instruction["input_ids"].to(device)
with torch.no_grad():
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        return_dict_in_generate=True,
        output_scores=True,
        repetition_penalty=1.2,
    )
    output = generation_output.sequences[0]
    output = tokenizer.decode(output, skip_special_tokens=True)
    print(output)

```

[TechGPT2-Atom Example:](https://github.com/neukg/TechGPT/blob/main/inference.py)

``` python
from transformers import LlamaTokenizer, AutoModelForCausalLM, AutoConfig, GenerationConfig
import torch


TEMPLATE = (
    "<s>Human: "
    "{instruction} \n</s><s>Assistant: "
)

def generate_prompt(instruction):
    return TEMPLATE.format_map({'instruction': instruction})

ckpt_path = './LLM/TechGPT2-Atom-hf/'
load_type = torch.float16
device = torch.device(1)
tokenizer = LlamaTokenizer.from_pretrained(ckpt_path)
tokenizer.pad_token_id = 2
tokenizer.bos_token_id = 1
tokenizer.eos_token_id = 2
tokenizer.padding_side = "left"
model_config = AutoConfig.from_pretrained(ckpt_path)
model = AutoModelForCausalLM.from_pretrained(ckpt_path, torch_dtype=load_type, config=model_config)
model.to(device)
model.eval()

generation_config = GenerationConfig(
    temperature=0.1,
    top_p=0.75,
    top_k=40,
    num_beams=1,
    bos_token_id=1,
    eos_token_id=2,
    pad_token_id=2,
    max_new_tokens=128,
    min_new_tokens=20,
    do_sample=True,
)

example = 'æŠ½å–å‡ºä¸‹é¢æ–‡æœ¬çš„å®ä½“å’Œå®ä½“ç±»å‹ï¼šã€Šå¥³äººæ ‘ã€‹ï¼Œå›½äº§ç”µè§†å‰§ï¼Œç”±å¯¼æ¼”ç”°è¿ªæ‰§å¯¼ï¼Œæ ¹æ®ä½œå®¶å­é¡µçš„åŸè‘—æ”¹ç¼–ï¼Œæ•…äº‹ä»1947å¹´å¼€å§‹ï¼Œè·¨è¶Šäº†è§£æ”¾æˆ˜äº‰å’Œå»ºå›½åˆæœŸä¸¤å¤§å†å²æ—¶æœŸï¼Œå±•ç°äº†æˆ˜æ–—åœ¨éšå½¢æˆ˜çº¿ä¸Šçš„äººæ°‘è‹±é›„æ˜¯å¦‚ä½•ä¸æƒ§æ€•ä»»ä½•å±é™©ï¼Œä¸è®¡è¾ƒä¸ªäººç‰ºç‰²ï¼Œç”šè‡³ä¸é¡¾äººæ°‘å†…éƒ¨çš„è¯¯è§£å’Œç”Ÿæ­»è£å†³ï¼Œéƒ½ä¸æš´éœ²ä¸ªäººçœŸå®èº«ä»½ï¼Œè‡³æ­»ä¸æ¸ï¼Œä¸æ•Œäººå‘¨æ—‹åˆ°åº•çš„è‹±é›„æ•…äº‹ã€‚'

instruction = generate_prompt(instruction=example)

instruction = tokenizer(instruction, return_tensors="pt")
input_ids = instruction["input_ids"].to(device)
with torch.no_grad():
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        return_dict_in_generate=True,
        output_scores=True,
        repetition_penalty=1.2,
    )
    output = generation_output.sequences[0]
    output = tokenizer.decode(output, skip_special_tokens=True)
    print(output)

```